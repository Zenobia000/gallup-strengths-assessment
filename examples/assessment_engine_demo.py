"""
Assessment Engine Demo
è©•æ¸¬å¼•æ“ç¤ºç¯„ç¨‹å¼

Author: Claude Code AI
Date: 2025-10-02
Version: 1.0
"""

import sys
import json
from pathlib import Path

# æ·»åŠ æ¨¡çµ„è·¯å¾‘
sys.path.append(str(Path(__file__).parent.parent / "src" / "main" / "python"))

from core.assessment import FileBasedAssessmentEngine


def demo_questionnaire_generation():
    """ç¤ºç¯„å•å·ç”Ÿæˆ"""
    print("=" * 60)
    print("å•å·ç”Ÿæˆç¤ºç¯„")
    print("=" * 60)

    # åˆå§‹åŒ–å¼•æ“
    engine = FileBasedAssessmentEngine()

    # ç”Ÿæˆç³»çµ±åŒ–å•å·
    print("\n1. ç”Ÿæˆç³»çµ±åŒ–è¼ªè½‰å•å·...")
    systematic_questionnaire = engine.generate_questionnaire(
        method="systematic_rotation",
        random_seed=42
    )

    print(f"âœ… å•å·ID: {systematic_questionnaire['questionnaire_id']}")
    print(f"âœ… ç¸½å€å¡Šæ•¸: {systematic_questionnaire['total_blocks']}")
    print(f"âœ… ç”Ÿæˆæ–¹æ³•: {systematic_questionnaire['generation_method']}")

    # é¡¯ç¤ºå‰3å€‹å€å¡Š
    print("\nå‰3å€‹å€å¡Šç¯„ä¾‹:")
    for i, block in enumerate(systematic_questionnaire['blocks'][:3]):
        print(f"\nå€å¡Š {block['block_id']}:")
        for j, stmt in enumerate(block['statements']):
            print(f"  {j+1}. [{stmt['talent']}] {stmt['text']}")

        validation = block['validation']
        print(f"  é©—è­‰: é ˜åŸŸæ•¸={validation['domain_count']}, é ˜åŸŸå¤šæ¨£æ€§={'âœ…' if validation['domain_diversity_met'] else 'âŒ'}")

    # é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
    stats = systematic_questionnaire['summary_statistics']
    print(f"\nğŸ“Š çµ±è¨ˆä¿¡æ¯:")
    print(f"  - æ‰å¹¹é »ç‡åˆ†å¸ƒ: {dict(list(stats['talent_frequency'].items())[:5])}...")
    print(f"  - å¹³è¡¡åˆ†æ•¸: {stats['balance_score']:.3f}")

    return systematic_questionnaire


def demo_heuristic_generation():
    """ç¤ºç¯„å•Ÿç™¼å¼ç”Ÿæˆ"""
    print("\n2. ç”Ÿæˆå•Ÿç™¼å¼å•å·...")
    engine = FileBasedAssessmentEngine()

    heuristic_questionnaire = engine.generate_questionnaire(
        method="heuristic",
        random_seed=42
    )

    print(f"âœ… å•å·ID: {heuristic_questionnaire['questionnaire_id']}")
    print(f"âœ… ç”Ÿæˆæ–¹æ³•: {heuristic_questionnaire['generation_method']}")

    # æ¯”è¼ƒå¹³è¡¡æ€§
    stats = heuristic_questionnaire['summary_statistics']
    print(f"ğŸ“Š å•Ÿç™¼å¼å¹³è¡¡åˆ†æ•¸: {stats['balance_score']:.3f}")

    return heuristic_questionnaire


def demo_mock_assessment():
    """ç¤ºç¯„æ¨¡æ“¬è©•æ¸¬"""
    print("\n=" * 60)
    print("æ¨¡æ“¬è©•æ¸¬ç¤ºç¯„")
    print("=" * 60)

    engine = FileBasedAssessmentEngine()

    # ç”Ÿæˆå•å·
    questionnaire = engine.generate_questionnaire(method="systematic_rotation")

    # æ¨¡æ“¬ç”¨æˆ¶å›æ‡‰ - åå¥½ T4 (åˆ†æèˆ‡æ´å¯Ÿ) å’Œ T1 (çµæ§‹åŒ–åŸ·è¡Œ)
    print("\nğŸ¤– æ¨¡æ“¬åå¥½åˆ†æå‹å’ŒåŸ·è¡Œå‹æ‰å¹¹çš„ç”¨æˆ¶...")
    mock_responses = []

    for block in questionnaire['blocks']:
        statements = block['statements']

        # ç­–ç•¥ï¼šå„ªå…ˆé¸æ“‡ T4, T1ï¼Œé¿å… T5, T7
        most_like = statements[0]  # é è¨­é¸ç¬¬ä¸€å€‹
        least_like = statements[-1]  # é è¨­é¸æœ€å¾Œä¸€å€‹

        # æ™ºèƒ½é¸æ“‡é‚è¼¯
        for stmt in statements:
            if stmt['talent'] in ['T4', 'T1']:  # åˆ†æèˆ‡åŸ·è¡Œ
                most_like = stmt
                break

        for stmt in reversed(statements):
            if stmt['talent'] in ['T5', 'T7']:  # å½±éŸ¿èˆ‡å®¢æˆ¶å°å‘
                least_like = stmt
                break

        mock_responses.append({
            "block_id": block['block_id'],
            "most_like": most_like['statement_id'],
            "least_like": least_like['statement_id']
        })

    # é©—è­‰å›æ‡‰
    validation = engine.validate_responses(mock_responses)
    print(f"âœ… å›æ‡‰é©—è­‰: {'é€šé' if validation['is_valid'] else 'å¤±æ•—'}")
    if validation['issues']:
        print(f"âŒ å•é¡Œ: {validation['issues']}")

    # è¨ˆç®—åˆ†æ•¸
    print("\nğŸ§® è¨ˆç®—è©•æ¸¬åˆ†æ•¸...")
    result = engine.calculate_scores(mock_responses)

    # é¡¯ç¤ºæ‰å¹¹åˆ†æ•¸
    print("\nğŸ“Š æ‰å¹¹åˆ†æ•¸æ’å (Top 5):")
    top_talents = sorted(result.talent_scores, key=lambda x: x.percentile, reverse=True)[:5]
    for i, talent in enumerate(top_talents):
        print(f"  {i+1}. {talent.talent_name} ({talent.talent_id}): {talent.percentile}%")

    # é¡¯ç¤ºé ˜åŸŸåˆ†æ•¸
    print("\nğŸ¯ é ˜åŸŸåˆ†æ•¸:")
    for domain in result.domain_scores:
        print(f"  {domain.domain_name}: {domain.percentile}%")

    # é¡¯ç¤ºå‡è¡¡æŒ‡æ¨™
    balance = result.balance_metrics
    print(f"\nâš–ï¸ å‡è¡¡æŒ‡æ¨™:")
    print(f"  - é ˜åŸŸå‡è¡¡æŒ‡æ•¸ (DBI): {balance.dbi:.3f}")
    print(f"  - ç›¸å°ç†µ: {balance.relative_entropy:.3f}")
    print(f"  - Giniä¿‚æ•¸: {balance.gini_coefficient:.3f}")
    print(f"  - æª”æ¡ˆé¡å‹: {balance.profile_type}")

    # é¡¯ç¤ºåŸå‹åˆ†é¡
    archetype = result.archetype_classification
    print(f"\nğŸ­ è·æ¥­åŸå‹åˆ†é¡:")
    print(f"  ä¸»è¦åŸå‹: {archetype.primary_archetype['archetype_name']} ({archetype.primary_archetype['probability']:.2f})")
    print(f"  æ¬¡è¦åŸå‹: {archetype.secondary_archetype['archetype_name']} ({archetype.secondary_archetype['probability']:.2f})")
    print(f"  ä¿¡å¿ƒåº¦: {archetype.primary_archetype['confidence']}")

    # é¡¯ç¤ºè·æ¥­å»ºè­°
    career_recs = archetype.career_recommendations
    print(f"\nğŸ’¼ è·æ¥­å»ºè­°:")
    print(f"  ä¸»è¦å»ºè­°: {', '.join(career_recs['primary_suggestions'])}")
    print(f"  æ›¿ä»£è·¯å¾‘: {', '.join(career_recs['alternative_paths'])}")

    print(f"\nğŸ¯ æ•´é«”ä¿¡å¿ƒåº¦: {result.confidence:.2f}")

    return result


def demo_benchmark_criteria():
    """ç¤ºç¯„è©•æ¸¬åŸºæº–"""
    print("\n=" * 60)
    print("è©•æ¸¬åŸºæº–ç¤ºç¯„")
    print("=" * 60)

    engine = FileBasedAssessmentEngine()

    # ç²å–åŸºæº–æ¨™æº–
    benchmarks = engine.get_benchmark_criteria()

    print("\nğŸ“ å¿ƒç†æ¸¬é‡å­¸æ¨™æº–:")
    reliability = benchmarks['psychometric_standards']['reliability']
    print(f"  Cronbach Alpha: æœ€ä½ {reliability['cronbach_alpha']['minimum']}, ç›®æ¨™ {reliability['cronbach_alpha']['target']}")

    validity = benchmarks['psychometric_standards']['validity']
    construct = validity['construct_validity']
    print(f"  å› å­è¼‰è·: æœ€ä½ {construct['factor_loading']['minimum']}, ç›®æ¨™ {construct['factor_loading']['target']}")

    print("\nâš¡ æŠ€è¡“æ•ˆèƒ½æ¨™æº–:")
    technical = benchmarks['technical_performance']
    print(f"  æ¯å€å¡Šå›æ‡‰æ™‚é–“: {technical['response_time']['per_block']['target']}")
    print(f"  å®Œæˆç‡ç›®æ¨™: {technical['completion_rate']['target'] * 100}%")

    print("\nâœ… å…¬å¹³æ€§æ¨™æº–:")
    fairness = benchmarks['psychometric_standards']['fairness']
    print(f"  DIF æœ€å¤§æ•ˆæ‡‰å€¼: {fairness['differential_item_functioning']['max_effect_size']}")


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ File-based Assessment Engine Demo")
    print("åŸºæ–¼æ–‡ä»¶å­˜å„²çš„è©•æ¸¬å¼•æ“ç¤ºç¯„")

    try:
        # 1. å•å·ç”Ÿæˆç¤ºç¯„
        systematic_q = demo_questionnaire_generation()
        heuristic_q = demo_heuristic_generation()

        # 2. æ¨¡æ“¬è©•æ¸¬ç¤ºç¯„
        assessment_result = demo_mock_assessment()

        # 3. è©•æ¸¬åŸºæº–ç¤ºç¯„
        demo_benchmark_criteria()

        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰ç¤ºç¯„å®Œæˆï¼")
        print("=" * 60)

        # è©¢å•æ˜¯å¦å°å‡ºçµæœ
        print("\nğŸ’¾ æ˜¯å¦å°å‡ºè©•æ¸¬çµæœåˆ° JSON æª”æ¡ˆï¼Ÿ(y/n): ", end="")

        # ç°¡åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥å°å‡º
        output_file = "assessment_result_demo.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "systematic_questionnaire": systematic_q,
                "assessment_result": assessment_result.__dict__
            }, f, indent=2, ensure_ascii=False, default=str)

        print(f"âœ… çµæœå·²å°å‡ºåˆ° {output_file}")

    except Exception as e:
        print(f"âŒ ç¤ºç¯„éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()